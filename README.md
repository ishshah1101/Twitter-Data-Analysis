# ğŸ› ï¸ Twitter Data Engineering Pipeline

An end-to-end data pipeline that extracts real-time tweets using the Twitter API, transforms and validates the data using Python, orchestrates the workflow via Apache Airflow, and loads it into AWS S3 â€” all deployed on an EC2 instance. Designed for scalable ingestion and monitoring of social media analytics.

## ğŸš€ Overview

This project automates the extraction, transformation, and loading (ETL) of Twitter data for downstream analytics. It leverages cloud infrastructure (AWS EC2 & S3) and open-source tools (Apache Airflow, Tweepy) to ensure a reliable and scalable data pipeline.

## ğŸ”§ Tech Stack

- **Python** â€“ Data extraction, transformation, and validation
- **Twitter API (Tweepy)** â€“ Real-time tweet streaming
- **Apache Airflow** â€“ Workflow orchestration and scheduling
- **AWS EC2** â€“ Hosting and execution environment
- **AWS S3** â€“ Data storage layer
- **Boto3** â€“ Python SDK for AWS services
- **Pandas** â€“ Data wrangling


## ğŸ”„ ETL Workflow

1. **Extract**: Fetch tweets using Tweepy and store as raw JSON.
2. **Transform**: Parse and clean tweet data (text, timestamp, hashtags, etc.).
3. **Load**: Upload transformed data to an S3 bucket in `.csv` format.
4. **Schedule**: Run pipeline daily/hourly using Apache Airflow DAG.
5. **Monitor**: Track task success/failure using the Airflow web UI.

## ğŸ“¦ Setup Instructions

1. Clone the repository:
   ```bash
   git clone https://github.com/your-username/twitter-etl-pipeline.git
   cd twitter-etl-pipeline


